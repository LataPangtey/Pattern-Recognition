{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "S6G6Vl7ZsJOE",
    "outputId": "9a894247-101b-4d77-c3a0-5a50ba7e8ad4"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# # Getting GPU device name.\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# if device_name == '/device:GPU:0':\n",
    "#     print('Found GPU at: {}'.format(device_name))\n",
    "# else:\n",
    "#     raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "DgYrSE67sLva",
    "outputId": "939b440f-d4a4-4020-dd9a-0b7068eb9986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# If a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    #set device to GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If no GPU is available\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760,
     "referenced_widgets": [
      "07b6b949f6a148d59bcef39cee3e77c8",
      "fb30347e7fe441bcb5ab7653ea1bac9f",
      "752cfdef7eec40cd8f34201e9d54d012",
      "009209791281479fbcd240f3ebfb5954",
      "0ea2cdb682c7477c8b1c3b1adc566243",
      "5e2f6a052b8044c79e097f9db3f46c18",
      "27925e9a9b4a4c3cb38e36db191481eb",
      "27a5bbd9f3de4ed08411521114318411",
      "1cfb509850884a3a97561996ec91ba8d",
      "3be36a54cccd4623ada280deaded70d2",
      "7fdb570b7e8d4709a7a6686cd4d57c65",
      "3f0c2810564842438058c41d005398fd",
      "2d3d8678b433422395363d2f7537e9ad",
      "af93498e584b4131b77410ed0e9d340c",
      "91fabef68bf24fc38e69caa580f98f6e",
      "0b67ce6ad9604aed9d52d6be2d1ad05c"
     ]
    },
    "id": "frFYhWeYsNx-",
    "outputId": "9bc84f14-b19f-47ef-ad30-4c78c19c68f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/lata/anaconda3/lib/python3.10/site-packages (4.40.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: requests in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: filelock in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lata/anaconda3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lata/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/lata/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lata/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lata/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lata/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lata/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "!pip install transformers\n",
    "\n",
    "import re\n",
    "import scipy\n",
    "import pandas         as pd\n",
    "import io\n",
    "import numpy          as np\n",
    "import copy\n",
    "import seaborn        as sns\n",
    "\n",
    "import transformers\n",
    "from transformers                     import  RobertaModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics                  import classification_report\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "\n",
    "from torch                            import nn, optim\n",
    "from torch.utils                      import data\n",
    "from sklearn.decomposition            import PCA\n",
    "\n",
    "#Seeding for deterministic results\n",
    "RANDOM_SEED = 64\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   torch.cuda.manual_seed(RANDOM_SEED)\n",
    "   torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "   torch.backends.cudnn.deterministic = True\n",
    "   torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "CLASS_NAMES = ['AGAINST','FAVOUR','NONE']\n",
    "MAX_LENGTH = 200\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 6\n",
    "HIDDEN_UNITS = 128\n",
    "\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-large')  #Use roberta-large or roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v1zp-jMcsWIz"
   },
   "outputs": [],
   "source": [
    "#Converting labels to numbers\n",
    "def label_to_int(label):\n",
    "  if label   == 'AGAINST':\n",
    "    return 0\n",
    "  elif label == 'FAVOR':\n",
    "    return 1\n",
    "  elif label == 'NONE':\n",
    "    return 2\n",
    "\n",
    "\n",
    "#Pre-processing Twitter and Reddit Posts to handle URLs and Mentions.\n",
    "#Replaces URLs with $URL$ and mentions with $MENTION$\n",
    "def processText(text):\n",
    "  text = re.sub(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \"$URL$\",text.strip())\n",
    "  text = re.sub(r\"(@[A-Za-z0-9]+)\", \"$MENTION$\", text.strip())\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QImtZoCNsYrw"
   },
   "outputs": [],
   "source": [
    "'''Processing all of Twitter and Reddit data frames to\n",
    "    1. Get rid of all NaN values\n",
    "    2. Remove columns not useful for the Model\n",
    "    3. Process text\n",
    "    4. Return a combined frame consisting of both Twitter and Reddit data'''\n",
    "\n",
    "\n",
    "def processStanceData(RedditDf):\n",
    "    # Concatenating Reddit data (you can add more frames if needed)\n",
    "    frames = [RedditDf]\n",
    "    resultDf = pd.concat(frames)\n",
    "\n",
    "    # Getting rid of NaN values\n",
    "    resultDf = resultDf.replace(np.nan, '', regex=True)\n",
    "    #print(\"resultDf##############\",resultDf)\n",
    "    # Converting labels to numbers\n",
    "    resultDf['labelValue'] = resultDf['Stance'].apply(label_to_int)\n",
    "\n",
    "    # Concatenating previousText and sourceText into previousPlusSrcText\n",
    "   \n",
    "\n",
    "   \n",
    "    # Processing text fields if needed\n",
    "    resultDf['Tweet'] = resultDf['Tweet'].apply(processText)\n",
    "    resultDf['Target'] = resultDf['Target'].apply(processText)\n",
    "\n",
    "    return resultDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files converted to CSV successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['ID', 'Target', 'Tweet', 'Stance', 'Opinion towards', 'Sentiment', 'labelValue']\n",
    "\n",
    "# Read the text files with specified column names\n",
    "redditTrainDf = pd.read_csv('semeval_train.txt', delimiter='\\t', header=None, names=column_names)\n",
    "redditDevDf = pd.read_csv('semeval_test.txt', delimiter='\\t', header=None, names=column_names)\n",
    "redditTestDf = pd.read_csv('semeval_test.txt', delimiter='\\t', header=None, names=column_names)\n",
    "\n",
    "# Remove any rows where 'ID' is not numeric (to handle repeated headers or invalid rows)\n",
    "redditTrainDf = redditTrainDf[pd.to_numeric(redditTrainDf['ID'], errors='coerce').notnull()]\n",
    "redditDevDf = redditDevDf[pd.to_numeric(redditDevDf['ID'], errors='coerce').notnull()]\n",
    "redditTestDf = redditTestDf[pd.to_numeric(redditTestDf['ID'], errors='coerce').notnull()]\n",
    "\n",
    "# Save the DataFrames to CSV files without the index\n",
    "redditTrainDf.to_csv('semeval_train.csv', index=False)\n",
    "redditDevDf.to_csv('semeval_dev.csv', index=False)\n",
    "redditTestDf.to_csv('semeval_test.csv', index=False)\n",
    "\n",
    "print(\"Files converted to CSV successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "Z4ARH8pisdYA",
    "outputId": "71116769-2964-4756-8422-68e3c2a251aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Opinion towards</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>labelValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ And, #HandOverTheServer she wiped cl...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>Hillary is our best choice if we truly want to...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ I think our country is ready for a f...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>I just gave an unhealthy amount of my hard-ear...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ Thank you for adding me to your list...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NO ONE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>2910</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>There's a law protecting unborn eagles, but no...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>2911</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>I am 1 in 3... I have had an abortion #Abortio...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEITHER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>2912</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>How dare you say my sexual preference is a cho...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>2913</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>Equal rights for those 'born that way', no rig...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>2914</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>#POTUS seals his legacy w/ 1/2 doz wins. The #...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2914 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                    Target  \\\n",
       "0        1           Hillary Clinton   \n",
       "1        2           Hillary Clinton   \n",
       "2        3           Hillary Clinton   \n",
       "3        4           Hillary Clinton   \n",
       "4        5           Hillary Clinton   \n",
       "...    ...                       ...   \n",
       "2909  2910  Legalization of Abortion   \n",
       "2910  2911  Legalization of Abortion   \n",
       "2911  2912  Legalization of Abortion   \n",
       "2912  2913  Legalization of Abortion   \n",
       "2913  2914  Legalization of Abortion   \n",
       "\n",
       "                                                  Tweet   Stance  \\\n",
       "0     $MENTION$ And, #HandOverTheServer she wiped cl...  AGAINST   \n",
       "1     Hillary is our best choice if we truly want to...    FAVOR   \n",
       "2     $MENTION$ I think our country is ready for a f...  AGAINST   \n",
       "3     I just gave an unhealthy amount of my hard-ear...  AGAINST   \n",
       "4     $MENTION$ Thank you for adding me to your list...     NONE   \n",
       "...                                                 ...      ...   \n",
       "2909  There's a law protecting unborn eagles, but no...  AGAINST   \n",
       "2910  I am 1 in 3... I have had an abortion #Abortio...  AGAINST   \n",
       "2911  How dare you say my sexual preference is a cho...  AGAINST   \n",
       "2912  Equal rights for those 'born that way', no rig...  AGAINST   \n",
       "2913  #POTUS seals his legacy w/ 1/2 doz wins. The #...  AGAINST   \n",
       "\n",
       "     Opinion towards Sentiment  labelValue  \n",
       "0             TARGET  NEGATIVE           0  \n",
       "1             TARGET  POSITIVE           1  \n",
       "2             TARGET  NEGATIVE           0  \n",
       "3             TARGET  NEGATIVE           0  \n",
       "4             NO ONE  POSITIVE           2  \n",
       "...              ...       ...         ...  \n",
       "2909          TARGET  NEGATIVE           0  \n",
       "2910           OTHER   NEITHER           0  \n",
       "2911           OTHER  NEGATIVE           0  \n",
       "2912           OTHER  NEGATIVE           0  \n",
       "2913           OTHER  NEGATIVE           0  \n",
       "\n",
       "[2914 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading Twitter and Reddit data (train, dev and test) onto dataFrames\n",
    "#twitterTrainDf  = pd.read_csv(io.StringIO(uploaded['TwitterTrainDataSrc.csv'].decode('utf-8')))\n",
    "redditTrainDf   = pd.read_csv('semeval_train.csv')\n",
    "\n",
    "#twitterDevDf    = pd.read_csv(io.StringIO(uploaded['TwitterDevDataSrc.csv'].decode('utf-8')))\n",
    "redditDevDf     = pd.read_csv('semeval_test.csv')\n",
    "\n",
    "#twitterTestDf   = pd.read_csv(io.StringIO(uploaded['TwitterTestDataSrc.csv'].decode('utf-8')))\n",
    "redditTestDf    = pd.read_csv('semeval_test.csv')\n",
    "\n",
    "#Processing Twitter and Reddit dataframe containig training data\n",
    "trainDf = processStanceData(redditTrainDf)\n",
    "trainDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "apfESmuDsh13",
    "outputId": "6d68d55e-690f-4a99-f571-8ee86cb4fead"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Opinion towards</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>labelValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10675</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>#mtp $MENTION$ How is deleting emails -part of...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10676</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ $MENTION$ AndrewWhyDoYouCareAboutWha...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10677</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>The white male vote is solidly GOP. The black ...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10678</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ big banker buds need to ratchet up t...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10679</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ Why should I believe you on this? Th...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>11245</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>$MENTION$ $MENTION$_six I followed him before ...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>11246</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>For he who avenges blood remembers, he does no...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEITHER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>11247</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>Life is sacred on all levels. Abortion does no...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEITHER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>11248</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>$MENTION$ U refer to \"WE\" which =\"YOU\" &amp; a min...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>11249</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>Al Robertson's mom #DuckDynasty  chose life as...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEITHER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1249 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                    Target  \\\n",
       "0     10675           Hillary Clinton   \n",
       "1     10676           Hillary Clinton   \n",
       "2     10677           Hillary Clinton   \n",
       "3     10678           Hillary Clinton   \n",
       "4     10679           Hillary Clinton   \n",
       "...     ...                       ...   \n",
       "1244  11245  Legalization of Abortion   \n",
       "1245  11246  Legalization of Abortion   \n",
       "1246  11247  Legalization of Abortion   \n",
       "1247  11248  Legalization of Abortion   \n",
       "1248  11249  Legalization of Abortion   \n",
       "\n",
       "                                                  Tweet   Stance  \\\n",
       "0     #mtp $MENTION$ How is deleting emails -part of...  AGAINST   \n",
       "1     $MENTION$ $MENTION$ AndrewWhyDoYouCareAboutWha...  AGAINST   \n",
       "2     The white male vote is solidly GOP. The black ...  AGAINST   \n",
       "3     $MENTION$ big banker buds need to ratchet up t...  AGAINST   \n",
       "4     $MENTION$ Why should I believe you on this? Th...  AGAINST   \n",
       "...                                                 ...      ...   \n",
       "1244  $MENTION$ $MENTION$_six I followed him before ...     NONE   \n",
       "1245  For he who avenges blood remembers, he does no...  AGAINST   \n",
       "1246  Life is sacred on all levels. Abortion does no...  AGAINST   \n",
       "1247  $MENTION$ U refer to \"WE\" which =\"YOU\" & a min...  AGAINST   \n",
       "1248  Al Robertson's mom #DuckDynasty  chose life as...  AGAINST   \n",
       "\n",
       "     Opinion towards Sentiment  labelValue  \n",
       "0              OTHER  NEGATIVE           0  \n",
       "1              OTHER  NEGATIVE           0  \n",
       "2              OTHER  NEGATIVE           0  \n",
       "3             TARGET  NEGATIVE           0  \n",
       "4              OTHER  NEGATIVE           0  \n",
       "...              ...       ...         ...  \n",
       "1244           OTHER  NEGATIVE           2  \n",
       "1245          TARGET   NEITHER           0  \n",
       "1246          TARGET   NEITHER           0  \n",
       "1247          TARGET  NEGATIVE           0  \n",
       "1248          TARGET   NEITHER           0  \n",
       "\n",
       "[1249 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Processing Twitter and Reddit dataframe containig development data\n",
    "devDf = processStanceData(redditDevDf)\n",
    "devDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "id": "7MFgrBAeskj_",
    "outputId": "68152650-c86c-4370-89b7-14b915ff3303"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Opinion towards</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>labelValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10675</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>#mtp $MENTION$ How is deleting emails -part of...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10676</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ $MENTION$ AndrewWhyDoYouCareAboutWha...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10677</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>The white male vote is solidly GOP. The black ...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10678</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ big banker buds need to ratchet up t...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10679</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>$MENTION$ Why should I believe you on this? Th...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>11245</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>$MENTION$ $MENTION$_six I followed him before ...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>11246</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>For he who avenges blood remembers, he does no...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEITHER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>11247</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>Life is sacred on all levels. Abortion does no...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEITHER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>11248</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>$MENTION$ U refer to \"WE\" which =\"YOU\" &amp; a min...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>11249</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>Al Robertson's mom #DuckDynasty  chose life as...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>NEITHER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1249 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                    Target  \\\n",
       "0     10675           Hillary Clinton   \n",
       "1     10676           Hillary Clinton   \n",
       "2     10677           Hillary Clinton   \n",
       "3     10678           Hillary Clinton   \n",
       "4     10679           Hillary Clinton   \n",
       "...     ...                       ...   \n",
       "1244  11245  Legalization of Abortion   \n",
       "1245  11246  Legalization of Abortion   \n",
       "1246  11247  Legalization of Abortion   \n",
       "1247  11248  Legalization of Abortion   \n",
       "1248  11249  Legalization of Abortion   \n",
       "\n",
       "                                                  Tweet   Stance  \\\n",
       "0     #mtp $MENTION$ How is deleting emails -part of...  AGAINST   \n",
       "1     $MENTION$ $MENTION$ AndrewWhyDoYouCareAboutWha...  AGAINST   \n",
       "2     The white male vote is solidly GOP. The black ...  AGAINST   \n",
       "3     $MENTION$ big banker buds need to ratchet up t...  AGAINST   \n",
       "4     $MENTION$ Why should I believe you on this? Th...  AGAINST   \n",
       "...                                                 ...      ...   \n",
       "1244  $MENTION$ $MENTION$_six I followed him before ...     NONE   \n",
       "1245  For he who avenges blood remembers, he does no...  AGAINST   \n",
       "1246  Life is sacred on all levels. Abortion does no...  AGAINST   \n",
       "1247  $MENTION$ U refer to \"WE\" which =\"YOU\" & a min...  AGAINST   \n",
       "1248  Al Robertson's mom #DuckDynasty  chose life as...  AGAINST   \n",
       "\n",
       "     Opinion towards Sentiment  labelValue  \n",
       "0              OTHER  NEGATIVE           0  \n",
       "1              OTHER  NEGATIVE           0  \n",
       "2              OTHER  NEGATIVE           0  \n",
       "3             TARGET  NEGATIVE           0  \n",
       "4              OTHER  NEGATIVE           0  \n",
       "...              ...       ...         ...  \n",
       "1244           OTHER  NEGATIVE           2  \n",
       "1245          TARGET   NEITHER           0  \n",
       "1246          TARGET   NEITHER           0  \n",
       "1247          TARGET  NEGATIVE           0  \n",
       "1248          TARGET   NEITHER           0  \n",
       "\n",
       "[1249 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Processing Twitter and Reddit dataframe containig test data\n",
    "testDf = processStanceData(redditTestDf)\n",
    "testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hF32c5TJsn7R"
   },
   "outputs": [],
   "source": [
    "#Creates a dataset which will be used to feed to RoBERTa\n",
    "class StanceDataset(data.Dataset):\n",
    "\n",
    "  def __init__(self, firstSeq, secondSeq, TextSrcInre, labelValue,  tokenizer, max_len):\n",
    "    self.firstSeq    = firstSeq      #First input sequence that will be supplied to RoBERTa\n",
    "    self.secondSeq   = secondSeq     #Second input sequence that will be supplied to RoBERTa\n",
    "    self.TextSrcInre = TextSrcInre   #Concatenation of reply+ previous+ src text to get features from 1 training example\n",
    "    self.labelValue  = labelValue    #label value for each training example in the dataset\n",
    "    self.tokenizer   = tokenizer     #tokenizer that will be used to tokenize input sequences (Uses BERT-tokenizer here)\n",
    "    self.max_len     = max_len       #Maximum length of the tokens from the input sequence that BERT needs to attend to\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labelValue)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    firstSeq    = str(self.firstSeq[item])\n",
    "    secondSeq   = str(self.secondSeq[item])\n",
    "    TextSrcInre = str(self.TextSrcInre[item])\n",
    "\n",
    "    #Encoding the first and the second sequence to a form accepted by RoBERTa\n",
    "    #RoBERTa does not use token_type_ids to distinguish the first sequence from the second sequnece.\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        firstSeq,\n",
    "        secondSeq,\n",
    "        max_length = self.max_len,\n",
    "        add_special_tokens= True,\n",
    "        truncation = True,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'firstSeq' : firstSeq,\n",
    "        'secondSeq' : secondSeq,\n",
    "        'TextSrcInre': TextSrcInre,\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'labelValue'  : torch.tensor(self.labelValue[item], dtype=torch.long)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QivInecFsomT"
   },
   "outputs": [],
   "source": [
    "#Creates a data loader\n",
    "def createDataLoader(dataframe, tokenizer, max_len, batch_size):\n",
    "  ds = StanceDataset(\n",
    "      firstSeq    = dataframe.Tweet.to_numpy(),\n",
    "      secondSeq   = dataframe.Target.to_numpy(),\n",
    "      TextSrcInre = dataframe.TextSrcInre.to_numpy(),\n",
    "      labelValue  = dataframe.labelValue.to_numpy(),\n",
    "      tokenizer   = tokenizer,\n",
    "      max_len     = max_len\n",
    "  )\n",
    "\n",
    "  return data.DataLoader(\n",
    "      ds,\n",
    "      batch_size  = batch_size,\n",
    "      shuffle     = True,\n",
    "      num_workers = 4\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oWy-fUVOstCt"
   },
   "outputs": [],
   "source": [
    "#Combining the reply, previous and source texts to get features for 1 training example\n",
    "trainDf['TextSrcInre'] = trainDf['Tweet'].str.cat(trainDf['Target'],sep=\" \")\n",
    "devDf['TextSrcInre']   = devDf['Tweet'].str.cat(devDf['Target'],sep=\" \")\n",
    "testDf['TextSrcInre']  = testDf['Tweet'].str.cat(testDf['Target'],sep=\" \")\n",
    "\n",
    "\n",
    "#Creating data loader for training data\n",
    "trainDataLoader        = createDataLoader(trainDf, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "\n",
    "#Creating data loader for development data\n",
    "developmentDataLoader  = createDataLoader(devDf, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "\n",
    "#Creating data loader for test data\n",
    "testDataLoader         = createDataLoader(testDf, tokenizer, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "z9ndmG1btoKq",
    "outputId": "bd9f49df-f957-4746-d519-fb4e0a2de3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.5, min_df=10, ngram_range=(1, 2))\n",
      "21\n",
      "(2914, 21)\n"
     ]
    }
   ],
   "source": [
    "#Instantiating the tf-idf vectorizer object\n",
    "tfidf = TfidfVectorizer(min_df = 10, max_df = 0.5, ngram_range=(1,2))\n",
    "\n",
    "xtrain = trainDf['Target'].tolist()\n",
    "x_train_feats = tfidf.fit(xtrain)\n",
    "print(x_train_feats)\n",
    "print(len(x_train_feats.get_feature_names_out()))\n",
    "\n",
    "\n",
    "x_train_transform = x_train_feats.transform(xtrain)\n",
    "tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(x_train_transform)).float()\n",
    "print(x_train_transform.shape)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=21)\n",
    "p = pca.fit(tfidf_transform_tensor)\n",
    "#print(p.shape)\n",
    "#print(p)\n",
    "X = p.transform(tfidf_transform_tensor)\n",
    "#torch.from_numpy(X.values)\n",
    "X = torch.from_numpy(X)\n",
    "#tfidf_transform_tensor_pca = torch.tensor(scipy.sparse.csr_matrix.todense(X)).float()\n",
    "#print(X.type())\n",
    "#print(X.shape)\n",
    "#print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZeET7Siptyco"
   },
   "outputs": [],
   "source": [
    "#This class defines the model that was used to pre-train a SNN on TF-IDF features\n",
    "class Tfidf_Nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden  = nn.Linear(len(tfidf.get_feature_names_out()), HIDDEN_UNITS)\n",
    "        # Output layer\n",
    "        self.output  =  nn.Linear(HIDDEN_UNITS, 3)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Defining tanh activation and softmax output\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        #print(x.shape)\n",
    "        y = self.tanh(x)\n",
    "        #print(y.shape)\n",
    "        z = self.dropout(y)\n",
    "        #print(z.shape)\n",
    "        z = self.output(z)\n",
    "        #print(z.shape)\n",
    "        z = self.softmax(z)\n",
    "\n",
    "        #Returning the ouputs from the hidden layer and the final output layer\n",
    "        return  y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "dZGZhYgbtz--",
    "outputId": "5151b3b1-045a-4c47-811d-ee32506d20e9"
   },
   "outputs": [],
   "source": [
    "#Loading the already trained MLP model that was trained on TF-IDF features.\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "snnmodel = Tfidf_Nn()\n",
    "\n",
    "# model_save_name = 'pre-trainedTfidf.pt'\n",
    "# path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "\n",
    "# snnmodel.load_state_dict(torch.load(path))\n",
    "# snnmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FMfWJLpssxRb"
   },
   "outputs": [],
   "source": [
    "'''This class defines the model that will be used for\n",
    "training and testing on the dataset.\n",
    "\n",
    "Adapted from huggingFace\n",
    "This RoBERTa model from huggingface outputs the last hidden states\n",
    "and the pooled output by default. Pooled output is the classification\n",
    "token (1st token of the last hidden state) further processed by a Linear\n",
    "layer and a Tanh activation function.\n",
    "\n",
    "The pre-trained RoBERTa model is used as the primary model.\n",
    "This class experiments with RoBERTa and its ensemble with TF-IDF features.\n",
    "roberta-only :            No ensembling. This just fine-tunes the RoBERTa model.\n",
    "                          The pooled output is passed through a linear layer and\n",
    "                          softmax function is finally used for preictions.\n",
    "\n",
    "roberta-tfIdf :           This model conatenates the 1st token of last-hidden layer\n",
    "                          from RoBERTa with TF-IDF features. Various ways of this\n",
    "                          concatenation was experimented (using pooled output instead\n",
    "                          of 1st token of last hidden layer etc)\n",
    "\n",
    "roberta-pcaTfidf :        This model concatenates the pooled output from\n",
    "                          RoBERTa with the PCA transformed vector.\n",
    "\n",
    "roberta-preTrainedTfIdf : This model concatenates the pooled output from\n",
    "                          RoBERTa with the hidden layer output from a pre-trained\n",
    "                          SNN that was trained on TF-IDF features.\n",
    "\n",
    "Used dropout to prevent over-fitting.'''\n",
    "\n",
    "class StanceClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self,  n_classes):\n",
    "    super(StanceClassifier, self).__init__()\n",
    "    self.robertaModel              = RobertaModel.from_pretrained('roberta-large')    #use roberta-large or roberta-base\n",
    "    self.model_TFIDF               = snnmodel                                        #Pre-trained SNN trained with TF-IDF features\n",
    "\n",
    "    self.drop                      = nn.Dropout(p = 0.3)\n",
    "\n",
    "    self.output                    = nn.Linear(self.robertaModel.config.hidden_size, n_classes)\n",
    "\n",
    "    self.input_size_tfidf_only     = self.robertaModel.config.hidden_size + len(tfidf.get_feature_names_out())\n",
    "    self.input_size_tfidf_pca      = self.robertaModel.config.hidden_size + HIDDEN_UNITS\n",
    "\n",
    "    self.dense                     = nn.Linear( self.input_size_tfidf_only,  self.input_size_tfidf_only)\n",
    "    self.out_proj                  = nn.Linear( self.input_size_tfidf_only, n_classes)\n",
    "    self.out_pca                   = nn.Linear( self.input_size_tfidf_pca, n_classes)\n",
    "\n",
    "    self.input_size_preTrain_tfidf = self.robertaModel.config.hidden_size +  HIDDEN_UNITS\n",
    "    self.out                       = nn.Linear(self.input_size_preTrain_tfidf, n_classes)\n",
    "\n",
    "    self.softmax                   = nn.Softmax(dim = 1)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, inputs_tfidf_feats, pca_transformed_feats, modelType):\n",
    "\n",
    "    roberta_output     = self.robertaModel(\n",
    "        input_ids      = input_ids,               #Input sequence tokens\n",
    "        attention_mask = attention_mask )         #Mask to avoid performing attention on padding tokens\n",
    "    #print(roberta_output[1].shape)\n",
    "\n",
    "    if modelType   == 'roberta-only':\n",
    "      pooled_output = roberta_output[1]           #Using pooled output\n",
    "      output        = self.drop(pooled_output)\n",
    "      output        = self.output(output)\n",
    "\n",
    "    elif modelType == 'roberta-tfIdf':\n",
    "      soutput = roberta_output[1]#---------        experimenting with pooled output\n",
    "      #soutput = roberta_output[0][:, 0, :]        #taking <s> token (equivalent to [CLS] token in BERT)\n",
    "      x       = torch.cat((soutput, inputs_tfidf_feats) , dim=1)\n",
    "      x       = self.drop(x)\n",
    "      output  = self.out_proj(x)\n",
    "\n",
    "    elif modelType == 'roberta-pcaTfidf':\n",
    "      soutput = roberta_output[1]\n",
    "      x       = torch.cat((soutput, pca_transformed_feats) , dim=1)\n",
    "      x       = self.drop(x)\n",
    "      output  = self.out_pca(x)\n",
    "\n",
    "    elif modelType == 'roberta-TrainedTfIdf':\n",
    "      tfidf_hidddenLayer, tfidf_output = self.model_TFIDF(inputs_tfidf_feats)\n",
    "      #print(tfidf_hidddenLayer.shape)\n",
    "      #print(tfidf_output.shape)\n",
    "\n",
    "      #Conactenating pooled output from RoBERTa with the hidden layer from the pre-trained SNN using TF-IDF features.\n",
    "      #pooled_output = torch.cat((roberta_output[1], tfidf_output) , dim=1)-------- Experimenting with Output of pre-trained SNN\n",
    "      pooled_output = torch.cat((roberta_output[1], tfidf_hidddenLayer) , dim=1)\n",
    "      output        = self.drop(pooled_output)\n",
    "      output        = self.out(output)\n",
    "\n",
    "    return self.softmax(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "8toH_ISooGBl",
    "outputId": "3779da82-2a2a-4029-8f77-f92d7ff6e59c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from google.colab import drive\\ndrive.mount(\\'/content/gdrive\\')\\nsnnmodel = Tfidf_Nn()\\n\\nmodel_save_name = \\'pre-trainedTfidf.pt\\'\\npath = F\"/content/gdrive/My Drive/{model_save_name}\"\\n\\nsnnmodel.load_state_dict(torch.load(path))\\nsnnmodel.eval()\\nmodel = StanceClassifier(len(CLASS_NAMES))\\n\\n#Loading fine-trained RoBERTa model on the same dataset\\nmodel_save_name = \\'RoBERTaLarge_TFIDFV2.pt\\'\\npath = F\"/content/gdrive/My Drive/{model_save_name}\"\\nmodel.load_state_dict(torch.load(path))\\nmodel.eval()\\nmodel = model.to(device)\\n\\n\\n# = StanceClassifier(len(CLASS_NAMES))\\n#model = model.to(device)\\nprint(model)\\n\\nprint(snnmodel)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "snnmodel = Tfidf_Nn()\n",
    "\n",
    "model_save_name = 'pre-trainedTfidf.pt'\n",
    "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "\n",
    "snnmodel.load_state_dict(torch.load(path))\n",
    "snnmodel.eval()\n",
    "model = StanceClassifier(len(CLASS_NAMES))\n",
    "\n",
    "#Loading fine-trained RoBERTa model on the same dataset\n",
    "model_save_name = 'RoBERTaLarge_TFIDFV2.pt'\n",
    "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# = StanceClassifier(len(CLASS_NAMES))\n",
    "#model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "print(snnmodel)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "e2b6db6195d144e4a3e848ad1757bfb4",
      "829bf6e8a65d4800bfd7287d6007c113",
      "6a95671548194fbca06ec1414b7bbc04",
      "707a75bbe4b941369233a16c829f8526",
      "cb1df164f46942ba8b2a6a2f910763aa",
      "af726520a9a14ec084faaf659f864310",
      "0a39426b7f594e59a481505bcf9768e0",
      "816082cb81da45f8a42dac691eab6b82",
      "1e940a261bfb4676aa5646d1fad7ebfc",
      "ae4795e9f23d47d382c726ab86965a9a",
      "ce7dc62473734b9ca8086e59513271af",
      "11066f3245804b359edef2938d1f11bc",
      "94f62d80d26a4f6f9c96983b4bae1008",
      "e0dffc10db3d41c4bc5da781f621ce67",
      "03350706a56841eba33d4b935a25ccb4",
      "4a0d78ddc6614653ab8f52ee49d3075f"
     ]
    },
    "id": "TmzNcXnMsx-p",
    "outputId": "42ac3655-e689-471b-8f86-4307563f70f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Instantiating a StanceClassifier object as our model and loading the model onto the GPU.\n",
    "model = StanceClassifier(len(CLASS_NAMES))\n",
    "model = model.to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z3mQd06Ns0d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''Using the same optimiser as used in BERT paper\n",
    "with a different learning rate'''\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-6,\n",
    "                  correct_bias= False)\n",
    "\n",
    "totalSteps = len(trainDataLoader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps = totalSteps\n",
    ")\n",
    "\n",
    "'''Using class-weights to accomodate heavily imbalanced data.\n",
    "These weights were learnt by running several experiments using\n",
    "other weights and the weights that produced the best results have\n",
    "finally been used here'''\n",
    "\n",
    "weights      = [8.0, 84.0, 8.0, 1.0]\n",
    "classWeights = torch.FloatTensor(weights)\n",
    "lossFunction = nn.CrossEntropyLoss(weight = classWeights).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FPJyrgGFs8TV"
   },
   "outputs": [],
   "source": [
    "#This function is used for training the model.\n",
    "def train_epoch(\n",
    "  model,\n",
    "  dataLoader,\n",
    "  lossFunction,\n",
    "  optimizer,\n",
    "  device,\n",
    "  scheduler,\n",
    "  n_examples\n",
    "):\n",
    "\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correctPredictions = 0\n",
    "\n",
    "  for d in dataLoader:\n",
    "\n",
    "    input_ids              = d[\"input_ids\"].to(device)                           #Loading input ids to GPU\n",
    "    attention_mask         = d[\"attention_mask\"].to(device)                      #Loading attention mask to GPU\n",
    "    labelValues            = d[\"labelValue\"].to(device)                          #Loading label value to GPU\n",
    "    textSrcInre            = d[\"TextSrcInre\"]\n",
    "    tfidf_transform        = x_train_feats.transform(textSrcInre)\n",
    "    tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()\n",
    "    pca_tensor             = p.transform(tfidf_transform_tensor)\n",
    "\n",
    "    pca_tensor = torch.from_numpy(pca_tensor).float()\n",
    "    pca_tensor = pca_tensor.to(device)\n",
    "    tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n",
    "\n",
    "    #Getting the output from our model (Object of StanceClassification class) for train data\n",
    "    outputs = model(\n",
    "      input_ids             = input_ids,\n",
    "      attention_mask        = attention_mask,\n",
    "      inputs_tfidf_feats    = tfidf_transform_tensor,\n",
    "      pca_transformed_feats = pca_tensor,\n",
    "      modelType             = 'roberta-TrainedTfIdf'\n",
    "    )\n",
    "\n",
    "    #Determining the model predictions\n",
    "    _, predictionIndices = torch.max(outputs, dim=1)\n",
    "    loss = lossFunction(outputs, labelValues)\n",
    "\n",
    "    #Calculating the correct predictions for accuracy\n",
    "    correctPredictions += torch.sum(predictionIndices == labelValues)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return np.mean(losses), correctPredictions.double() / n_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "a7kyxOzNs_33"
   },
   "outputs": [],
   "source": [
    "#This function is used for evaluating the model on the development and test set\n",
    "def eval_model(\n",
    "    model,\n",
    "    dataLoader,\n",
    "    lossFunction,\n",
    "    device,\n",
    "    n_examples\n",
    "    ):\n",
    "\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correctPredictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in dataLoader:\n",
    "\n",
    "      input_ids              = d[\"input_ids\"].to(device)                          #Loading input ids to GPU\n",
    "      attention_mask         = d[\"attention_mask\"].to(device)                     #Loading attention mask to GPU\n",
    "      labelValues            = d[\"labelValue\"].to(device)                         #Loading label values to GPU\n",
    "      textSrcInre            = d[\"TextSrcInre\"]\n",
    "      tfidf_transform        = x_train_feats.transform(textSrcInre)\n",
    "      tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()\n",
    "\n",
    "      pca_tensor             = p.transform(tfidf_transform_tensor)\n",
    "\n",
    "      pca_tensor = torch.from_numpy(pca_tensor).float()\n",
    "      pca_tensor = pca_tensor.to(device)\n",
    "      tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n",
    "\n",
    "      #Getting the softmax output from model for dev data\n",
    "      outputs = model(\n",
    "        input_ids             = input_ids,\n",
    "        attention_mask        = attention_mask,\n",
    "        inputs_tfidf_feats    = tfidf_transform_tensor,\n",
    "        pca_transformed_feats = pca_tensor,\n",
    "        modelType             = 'roberta-TrainedTfIdf'\n",
    "      )\n",
    "\n",
    "      #Determining the model predictions\n",
    "      _, predictionIndices = torch.max(outputs, dim=1)\n",
    "      loss = lossFunction(outputs, labelValues)\n",
    "\n",
    "      #Calculating the correct predictions for accuracy\n",
    "      correctPredictions += torch.sum(predictionIndices == labelValues)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return np.mean(losses), correctPredictions.double() / n_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "cm-MoWCftEQ4",
    "outputId": "a6a8befa-722b-415c-e29c-997e5ead3418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 1.187934927407279 Training accuracy 0.3291008922443377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 1.121121697151623 Development accuracy 0.5796637309847877\n",
      "\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 1.1147103273655978 Training accuracy 0.5343170899107755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 1.0310820645798509 Development accuracy 0.5524419535628502\n",
      "\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 1.0184522524619135 Training accuracy 0.6156485929993136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 1.002348232193115 Development accuracy 0.6965572457966372\n",
      "\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.9691773465794954 Training accuracy 0.7223747426218257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.9903921208823451 Development accuracy 0.7029623698959167\n",
      "\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.9231210699297273 Training accuracy 0.7786547700754975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.9690938772865758 Development accuracy 0.7277822257806245\n",
      "\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.9159954028365053 Training accuracy 0.7961564859299931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.9721457230778167 Development accuracy 0.7245796637309847\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fine tuning ROBERTa and validating it\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1}')\n",
    "  trainLoss, trainAccuracy = train_epoch(\n",
    "    model,\n",
    "    trainDataLoader,\n",
    "    lossFunction,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(trainDf)\n",
    "  )\n",
    "\n",
    "  print(f'Training loss {trainLoss} Training accuracy {trainAccuracy}')\n",
    "\n",
    "  devLoss, devAccuracy = eval_model(\n",
    "    model,\n",
    "    developmentDataLoader,\n",
    "    lossFunction,\n",
    "    device,\n",
    "    len(devDf)\n",
    "  )\n",
    "\n",
    "  print(f'Development loss {devLoss} Development accuracy {devAccuracy}')\n",
    "  print()\n",
    "\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "O_wi9oVXtHR9"
   },
   "outputs": [],
   "source": [
    "#This function gets the predictions from the model after it is trained.\n",
    "def get_predictions(model, data_loader):\n",
    "\n",
    "  model = model.eval()\n",
    "  review_texta = []\n",
    "  review_textb = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      textas                 = d[\"firstSeq\"]\n",
    "      textbs                 = d[\"secondSeq\"]\n",
    "      input_ids              = d[\"input_ids\"].to(device)\n",
    "      attention_mask         = d[\"attention_mask\"].to(device)\n",
    "      labels                 = d[\"labelValue\"].to(device)\n",
    "      textSrcInre            = d[\"TextSrcInre\"]\n",
    "      tfidf_transform        = tfidf.transform(textSrcInre)\n",
    "      tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()\n",
    "\n",
    "      pca_tensor             =  p.transform(tfidf_transform_tensor)\n",
    "\n",
    "      pca_tensor = torch.from_numpy(pca_tensor).float()\n",
    "      pca_tensor = pca_tensor.to(device)\n",
    "      tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n",
    "\n",
    "      #Getting the softmax output from model\n",
    "      outputs = model(\n",
    "        input_ids             = input_ids,\n",
    "        attention_mask        = attention_mask,\n",
    "        inputs_tfidf_feats    = tfidf_transform_tensor,\n",
    "        pca_transformed_feats = pca_tensor,\n",
    "        modelType             = 'roberta-TrainedTfIdf'\n",
    "      )\n",
    "\n",
    "      _, preds = torch.max(outputs, dim=1)     #Determining the model predictions\n",
    "\n",
    "      review_texta.extend(textas)\n",
    "      review_textb.extend(textbs)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs)\n",
    "      real_values.extend(labels)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "  return review_texta, review_textb, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hG_JcQt20reh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Getting model predictions on dev dataset\n",
    "firstSeq_dev, secondSeq_dev, yHat_dev, predProbs_dev, yTest_dev = get_predictions(\n",
    "  model,\n",
    "  developmentDataLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "nh219SUW0-Eh",
    "outputId": "7d0876ba-44a7-4922-917b-fec848ea6de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     AGAINST       0.88      0.69      0.77       715\n",
      "      FAVOUR       0.59      0.84      0.69       304\n",
      "        NONE       0.62      0.70      0.65       230\n",
      "\n",
      "    accuracy                           0.72      1249\n",
      "   macro avg       0.70      0.74      0.71      1249\n",
      "weighted avg       0.76      0.72      0.73      1249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  #Printing classification report for dev dataset (Evaluating the model on Dev set)\n",
    "CLASS_NAMES = ['AGAINST','FAVOUR','NONE']\n",
    "print(classification_report(yTest_dev, yHat_dev, target_names= CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmraXgpo1TAg"
   },
   "outputs": [],
   "source": [
    "# #Saving the model onto the drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# model_save_name = 'RoBERTaLarge_TFIDFV2.pt'\n",
    "# path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "# torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mmv7XMPZtIN3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/lata/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Getting model predictions on test dataset\n",
    "firstSeq_test, secondSeq_test, yHat_test, predProbs_test, yTest_test = get_predictions(\n",
    "  model,\n",
    "  testDataLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "QRwqVzOk0y7x",
    "outputId": "2e198a7c-90aa-4014-95a0-1f0b8f531130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     AGAINST       0.88      0.69      0.77       715\n",
      "      FAVOUR       0.59      0.84      0.69       304\n",
      "        NONE       0.62      0.70      0.65       230\n",
      "\n",
      "    accuracy                           0.72      1249\n",
      "   macro avg       0.70      0.74      0.71      1249\n",
      "weighted avg       0.76      0.72      0.73      1249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing classification report for test dataset (Evaluating the model on test set)\n",
    "print(classification_report(yTest_test, yHat_test, target_names= CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wYTempX1Gcj"
   },
   "outputs": [],
   "source": [
    "#Saving the predictions onto a CSV file for error analysis\n",
    "zippedList =  list(zip(firstSeq_test, secondSeq_test, yHat_test, predProbs_test, yTest_test ))\n",
    "dfObj = pd.DataFrame(zippedList, columns = ['Texta' , 'Textb', 'Ypred', 'YpredsProbs', 'label'])\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('drive')\n",
    "\n",
    "dfObj.to_csv('dataPredsFromRoberta_TFIDFV2.csv')\n",
    "!cp dataPredsFromRoberta_TFIDFV2.csv \"drive/My Drive/\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "009209791281479fbcd240f3ebfb5954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27a5bbd9f3de4ed08411521114318411",
      "placeholder": "​",
      "style": "IPY_MODEL_27925e9a9b4a4c3cb38e36db191481eb",
      "value": " 899k/899k [00:01&lt;00:00, 883kB/s]"
     }
    },
    "03350706a56841eba33d4b935a25ccb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07b6b949f6a148d59bcef39cee3e77c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_752cfdef7eec40cd8f34201e9d54d012",
       "IPY_MODEL_009209791281479fbcd240f3ebfb5954"
      ],
      "layout": "IPY_MODEL_fb30347e7fe441bcb5ab7653ea1bac9f"
     }
    },
    "0a39426b7f594e59a481505bcf9768e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b67ce6ad9604aed9d52d6be2d1ad05c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ea2cdb682c7477c8b1c3b1adc566243": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "11066f3245804b359edef2938d1f11bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a0d78ddc6614653ab8f52ee49d3075f",
      "placeholder": "​",
      "style": "IPY_MODEL_03350706a56841eba33d4b935a25ccb4",
      "value": " 1.43G/1.43G [01:10&lt;00:00, 20.1MB/s]"
     }
    },
    "1cfb509850884a3a97561996ec91ba8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7fdb570b7e8d4709a7a6686cd4d57c65",
       "IPY_MODEL_3f0c2810564842438058c41d005398fd"
      ],
      "layout": "IPY_MODEL_3be36a54cccd4623ada280deaded70d2"
     }
    },
    "1e940a261bfb4676aa5646d1fad7ebfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce7dc62473734b9ca8086e59513271af",
       "IPY_MODEL_11066f3245804b359edef2938d1f11bc"
      ],
      "layout": "IPY_MODEL_ae4795e9f23d47d382c726ab86965a9a"
     }
    },
    "27925e9a9b4a4c3cb38e36db191481eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27a5bbd9f3de4ed08411521114318411": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d3d8678b433422395363d2f7537e9ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3be36a54cccd4623ada280deaded70d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f0c2810564842438058c41d005398fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b67ce6ad9604aed9d52d6be2d1ad05c",
      "placeholder": "​",
      "style": "IPY_MODEL_91fabef68bf24fc38e69caa580f98f6e",
      "value": " 456k/456k [00:00&lt;00:00, 1.26MB/s]"
     }
    },
    "4a0d78ddc6614653ab8f52ee49d3075f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e2f6a052b8044c79e097f9db3f46c18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a95671548194fbca06ec1414b7bbc04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af726520a9a14ec084faaf659f864310",
      "max": 482,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb1df164f46942ba8b2a6a2f910763aa",
      "value": 482
     }
    },
    "707a75bbe4b941369233a16c829f8526": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_816082cb81da45f8a42dac691eab6b82",
      "placeholder": "​",
      "style": "IPY_MODEL_0a39426b7f594e59a481505bcf9768e0",
      "value": " 482/482 [01:11&lt;00:00, 6.77B/s]"
     }
    },
    "752cfdef7eec40cd8f34201e9d54d012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e2f6a052b8044c79e097f9db3f46c18",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ea2cdb682c7477c8b1c3b1adc566243",
      "value": 898823
     }
    },
    "7fdb570b7e8d4709a7a6686cd4d57c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af93498e584b4131b77410ed0e9d340c",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d3d8678b433422395363d2f7537e9ad",
      "value": 456318
     }
    },
    "816082cb81da45f8a42dac691eab6b82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "829bf6e8a65d4800bfd7287d6007c113": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91fabef68bf24fc38e69caa580f98f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94f62d80d26a4f6f9c96983b4bae1008": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ae4795e9f23d47d382c726ab86965a9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af726520a9a14ec084faaf659f864310": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af93498e584b4131b77410ed0e9d340c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb1df164f46942ba8b2a6a2f910763aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ce7dc62473734b9ca8086e59513271af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0dffc10db3d41c4bc5da781f621ce67",
      "max": 1425941629,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_94f62d80d26a4f6f9c96983b4bae1008",
      "value": 1425941629
     }
    },
    "e0dffc10db3d41c4bc5da781f621ce67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2b6db6195d144e4a3e848ad1757bfb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a95671548194fbca06ec1414b7bbc04",
       "IPY_MODEL_707a75bbe4b941369233a16c829f8526"
      ],
      "layout": "IPY_MODEL_829bf6e8a65d4800bfd7287d6007c113"
     }
    },
    "fb30347e7fe441bcb5ab7653ea1bac9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
